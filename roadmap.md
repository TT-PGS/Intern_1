# Roadmap

## ğŸ¯ Má»¤C TIÃŠU 1: XÃ¢y dá»±ng framework RL scheduling Ä‘Æ¡n giáº£n

### ğŸ”¹ Giai Ä‘oáº¡n A.1 â€“ MÃ´ hÃ¬nh bÃ i toÃ¡n & mÃ´i trÆ°á»ng

**âœ” Files cáº§n hoÃ n thiá»‡n:**

* `base/model.py`: class `SchedulingModel`
* `envs/simple_split_env.py`: mÃ´i trÆ°á»ng `SimpleSplitSchedulingEnv`

**ğŸ¯ YÃªu cáº§u:**

* MÃ´ hÃ¬nh cÃ³ thá»ƒ config dá»… dÃ ng (`num_jobs`, `num_machines`, `max_job_size`)
* MÃ´i trÆ°á»ng tráº£ state, nháº­n action, tÃ­nh reward

**ğŸ§ª CÃ¡ch test:**

* Táº¡o `test_env.py`: reset + step + in ra reward/makespan

---

### ğŸ”¹ Giai Ä‘oáº¡n A.2 â€“ Interface Agent + Agent Random

**âœ” Files cáº§n hoÃ n thiá»‡n:**

* `base/agent_base.py`
* `agents/random_agent.py`

**ğŸ¯ YÃªu cáº§u:**

* Agent tuÃ¢n theo `AgentBase`
* Agent random dÃ¹ng `action_dim` Ä‘á»ƒ chá»n action

**ğŸ§ª CÃ¡ch test:**

* DÃ¹ng `main.py` cháº¡y env + random agent â†’ tháº¥y reward in ra lÃ  Ä‘á»§

---

### ğŸ”¹ Giai Ä‘oáº¡n A.3 â€“ I/O handler + run loop

**âœ” Files cáº§n hoÃ n thiá»‡n:**

* `base/io_handler.py`
* `base/runner.py`

**ğŸ¯ YÃªu cáº§u:**

* `get_input()` vÃ  `show_output()` dá»… override
* `run_episode()` thá»±c hiá»‡n RL loop: reset, step, agent.select/update, return total reward

**ğŸ§ª CÃ¡ch test:**

* Gá»i `run_episode(env, agent, io)` tá»« `main.py`, in ra tá»•ng reward

---

### ğŸ”¹ Giai Ä‘oáº¡n A.4 â€“ Agent factory + config

**âœ” Files cáº§n hoÃ n thiá»‡n:**

* `agents/agent_factory.py`
* `configs/default_model.yaml`

**ğŸ¯ YÃªu cáº§u:**

* Agent Ä‘Æ°á»£c load tá»« config (vÃ­ dá»¥ `agent: random`)
* Config dá»… chá»‰nh sá»‘ job, sá»‘ mÃ¡y

**ğŸ§ª CÃ¡ch test:**

* Sá»­a `main.py` gá»i qua config, Ä‘á»•i agent dá»… dÃ ng

---

### ğŸ”¹ Giai Ä‘oáº¡n A.5 â€“ Log Ä‘Æ¡n giáº£n

**âœ” Files cáº§n viáº¿t:**

* `logger/structured_logger.py` (chÆ°a cáº§n exporters)
* Ghi JSON/line má»—i episode

**ğŸ§ª CÃ¡ch test:**

* Sau má»—i `run_episode()`, log ra `logs/YYYYMMDD-HHMM/train.jsonl`

---

## âœ… Káº¿t quáº£ Má»¥c tiÃªu 1

* Cháº¡y `main.py` â†’ gá»i Ä‘Æ°á»£c Ä‘Ãºng env, agent, log reward theo táº­p
* Output rÃµ rÃ ng, agent cÃ³ thá»ƒ dá»… dÃ ng thay

---

## ğŸ¯ Má»¤C TIÃŠU 2: TÃ­ch há»£p Deep Q-Learning (DQN)

### ğŸ”¹ Giai Ä‘oáº¡n B.1 â€“ Hiá»‡n thá»±c agent DQN

**âœ” Files:**

* `agents/dqn_agent.py` (má»›i)
* DÃ¹ng `torch`, `replay buffer`, `target network`

**ğŸ¯ YÃªu cáº§u:**

* Network chá»n action theo Q-value
* Update Q theo Bellman (target Q)
* CÃ³ epsilon-greedy

---

### ğŸ”¹ Giai Ä‘oáº¡n B.2 â€“ TÃ­ch há»£p agent vÃ o factory

* Sá»­a `agent_factory.py` Ä‘á»ƒ thÃªm `"dqn": DQNAgent(...)`

---

### ğŸ”¹ Giai Ä‘oáº¡n B.3 â€“ Tuning & log

* Ghi log reward qua nhiá»u episode
* CÃ³ thá»ƒ lÆ°u mÃ´ hÃ¬nh (checkpoint)

---

## ğŸ¯ Má»¤C TIÃŠU 3: TÃ­ch há»£p PPO (policy-based)

### ğŸ”¹ Giai Ä‘oáº¡n C.1 â€“ DÃ¹ng `stable-baselines3` (hoáº·c viáº¿t PPO riÃªng)

* Viáº¿t `agents/ppo_agent.py`: wrapper cá»§a SB3 PPO agent

**ğŸ¯ YÃªu cáº§u:**

* DÃ¹ng chuáº©n `gym.Env` Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i SB3
* Huáº¥n luyá»‡n qua `model.learn()`, predict action

---

### ğŸ”¹ Giai Ä‘oáº¡n C.2 â€“ So sÃ¡nh káº¿t quáº£

* Ghi log reward â†’ Grafana/ClickHouse
* So sÃ¡nh reward/makespan giá»¯a DQN vs PPO

---

## âœ… Tá»•ng káº¿t: CÃ¡c má»‘c Ä‘Ã¡nh giÃ¡

| Má»¥c tiÃªu   | ThÃ nh pháº©m rÃµ rÃ ng              | Äo lÆ°á»ng káº¿t quáº£              |
| ---------- | ------------------------------- | ----------------------------- |
| Má»¥c tiÃªu 1 | `main.py` cháº¡y random agent     | CÃ³ reward, makespan, log JSON |
| Má»¥c tiÃªu 2 | `DQNAgent` tÃ­ch há»£p vÃ  training | Log reward tÄƒng dáº§n theo táº­p  |
| Má»¥c tiÃªu 3 | `PPOAgent` tÃ­ch há»£p             | So sÃ¡nh PPO vs DQN báº±ng log   |
